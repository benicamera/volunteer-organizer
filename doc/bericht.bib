%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Descr:       Vorlage für Berichte der DHBW-Karlsruhe, BibTeX
%% Author:      Prof. Dr. Jürgen Vollmer, vollmer AT dhbw-karlsruhe.de
%% $Id: bericht.bib,v 1.10 2016/03/16 12:27:42 vollmer draft $
%% -*- coding: utf-8 -*-
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%
%%  declararion of abbreviations
%%%%
@STRING{addison		= "Addison-Wesley"}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@BOOK{knuth.1984a,
AUTHOR		= "Knuth, Donald E.",
TITLE		= "The \TeX{}book",
PUBLISHER	= addison,
YEAR		= 1984
}

@BOOK{lamport.1995a,
AUTHOR		= "Lamport, Leslie",
TITLE		= "Das \LaTeX\ Handbuch",
PUBLISHER	= addison,
YEAR		= 1995
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Referenzieren von URL's
@MISC{dante.2010a,
  AUTHOR	= "Dante",
  TITLE		= "Webseite der \emph{Deutschsprachige Anwendervereinigung TeX e.V.}",
  HOWPUBLISHED 	= "\url{http://www.dante.de}",
  YEAR		= 2010,
  MONTH		= jan
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% This file was created with Citavi 6.17.0.0

@misc{Antrophic.2023,
 author = {Antrophic},
 date = {2023},
 title = {Model Card and Evaluations for Claude Models},
 url = {https://www-files.anthropic.com/production/images/ModelCardClaude2_with_appendix.pdf?dm=1700589594},
 keywords = {GenAI}
}

% This file was created with Citavi 6.17.0.0

@misc{Wright.,
 author = {Wright, Russel M.},
 title = {Constitutional AI: Implementation Tracker},
 url = {https://www.constitutional.ai/},
 keywords = {GenAI},
 urldate = {06.01.2024}
}

@article{falcon,
  title={The Falcon Series of Language Models: Towards Open Frontier Models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Alhammadi, Maitha and Daniele, Mazzotta and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@misc{Chung.2022,
  doi = {10.48550/ARXIV.2210.11416},
  
  url = {https://arxiv.org/abs/2210.11416},
  
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Instruction-Finetuned Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

% This file was created with Citavi 6.17.0.0

@misc{OpenAI.,
 author = {{Open AI}},
 title = {Models: GPT-4 and GPT-4 Turbo},
 url = {https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo},
 keywords = {GenAI}
}

% This file was created with Citavi 6.17.0.0

@misc{Meta.,
 author = {Meta},
 title = {Introducing Llama 2: The next generation of our open source large language model},
 url = {https://ai.meta.com/llama/},
 keywords = {GenAI}
}

% This file was created with Citavi 6.17.0.0

@misc{Jiang.10.10.2023,
 abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
 author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and {Le Scao}, Teven and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
 date = {10.10.2023},
 title = {Mistral 7B},
 url = {http://arxiv.org/pdf/2310.06825v1}
}

@article{wang2023openchat,
  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}

% This file was created with Citavi 6.17.0.0

@misc{Google.,
 author = {Google},
 title = {PaLM 2-Modelle},
 url = {https://developers.generativeai.google/products/palm},
 keywords = {GenAI},
 urldate = {17.11.2023}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609} 
,
  year={2023}
}

% This file was created with Citavi 6.17.0.0

@misc{Zheng.09.06.2023,
 abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80{\%} agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm{\_}judge.},
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
 date = {09.06.2023},
 title = {Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena},
 url = {http://arxiv.org/pdf/2306.05685v4}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244} 
,
  year={2023}
}

% This file was created with Citavi 6.17.0.0

@misc{Podell.05.07.2023,
 abstract = {We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models},
 author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
 date = {05.07.2023},
 title = {SDXL: Improving Latent Diffusion Models for High-Resolution Image  Synthesis},
 url = {http://arxiv.org/pdf/2307.01952v1},
 keywords = {GenAI}
}

% This file was created with Citavi 6.17.0.0

@misc{Betker.2023,
 author = {Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and Manassra, Wesam and Dhariwal, Prafulla and Chu, Casey and Jiao, Yunxin and Ramesh, Aditya},
 date = {2023},
 title = {Improving Image Generation with Better Captions},
 url = {https://cdn.openai.com/papers/dall-e-3.pdf},
 keywords = {GenAI},
 institution = {{Open AI}}
}

% This file was created with Citavi 6.17.0.0

@misc{Zheng.,
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan},
 title = {Chatbot Arena Leaderboard},
 url = {https://chat.lmsys.org/},
 keywords = {GenAI}
}
